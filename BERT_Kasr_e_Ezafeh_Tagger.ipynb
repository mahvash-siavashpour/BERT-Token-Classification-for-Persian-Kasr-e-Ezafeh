{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvF2EA2wlYIe"
      },
      "source": [
        "# Token Classification for Persian Kasr-e-Ezafeh"
      ],
      "id": "EvF2EA2wlYIe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2q2YUDGjpi9",
        "outputId": "ba4a6254-4ae3-4542-eac3-651aecf75eb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "n2q2YUDGjpi9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9I99btjllMU"
      },
      "source": [
        "Reading the dataset and parsing the data"
      ],
      "id": "a9I99btjllMU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f66cca80"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def read_wnut(file_path):\n",
        "    file_path = Path(file_path)\n",
        "    cnt = 0\n",
        "    raw_text = file_path.read_text('utf-8').strip()\n",
        "    cnt = 0\n",
        "    tokens = []\n",
        "    tags = []\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for line in raw_text.split('\\n'):\n",
        "        token, tag = line.split('\\t')\n",
        "        if token == '.':\n",
        "            cnt += 1\n",
        "            \n",
        "        tokens.append(token)\n",
        "        tags.append(tag)\n",
        "        if cnt == 3:\n",
        "            token_docs.append(tokens)\n",
        "            tag_docs.append(tags)\n",
        "            cnt = 0\n",
        "            tokens = []\n",
        "            tags = []\n",
        "\n",
        "    return token_docs, tag_docs\n",
        "\n",
        "texts, tags = read_wnut('drive/MyDrive/train_data.txt')"
      ],
      "id": "f66cca80"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1309957a",
        "outputId": "08441671-b173-4e9e-8370-1fe10586c3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample words: ['#', 'منبع', ':', ')', 'مجله', 'سروش', 'هفتگی', '،', 'مصاحبه', 'با']\n",
            "sample tags: ['O', 'O', 'O', 'O', 'ye', 'e', 'O', 'O', 'O', 'O']\n",
            "Total Words: 88571\n"
          ]
        }
      ],
      "source": [
        "print(f\"sample words: {texts[0][:10]}\", f\"sample tags: {tags[0][:10]}\", sep='\\n')\n",
        "print(f\"Total Words: {len(texts)}\")"
      ],
      "id": "1309957a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5c341a9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.1, train_size=.5)"
      ],
      "id": "c5c341a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "567440b3"
      },
      "outputs": [],
      "source": [
        "unique_tags = set(tag for doc in tags for tag in doc)\n",
        "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
        "id2tag = {id: tag for tag, id in tag2id.items()}"
      ],
      "id": "567440b3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cf68929",
        "outputId": "8ae36f11-407b-4ec5-87f9-28d132c85e67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'@e', 'O', 'e', 've', 'y', 'ye'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_tags"
      ],
      "id": "7cf68929"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd533d72",
        "outputId": "6d6a2095-fe8e-4703-edd4-9d05a61d9edd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: '@e', 1: 've', 2: 'ye', 3: 'y', 4: 'O', 5: 'e'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id2tag"
      ],
      "id": "dd533d72"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arWgQRvFmvZR"
      },
      "source": [
        "## Preprocessing the data"
      ],
      "id": "arWgQRvFmvZR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8907900f"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"HooshvareLab/distilbert-fa-zwnj-base\")\n",
        "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
      ],
      "id": "8907900f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggCsN9e9ndRR"
      },
      "source": [
        "Encoding the tags"
      ],
      "id": "ggCsN9e9ndRR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5aa049b"
      },
      "outputs": [],
      "source": [
        "label_all_tokens = True\n",
        "import numpy as np\n",
        "\n",
        "def encode_tags(tags, tokenized_inputs):\n",
        "    labels = []\n",
        "    tag_labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
        "    for i, label in enumerate(tag_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    return labels\n",
        "\n"
      ],
      "id": "a5aa049b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30699a3d"
      },
      "outputs": [],
      "source": [
        "train_labels = encode_tags(train_tags, train_encodings)\n",
        "val_labels = encode_tags(val_tags, val_encodings)"
      ],
      "id": "30699a3d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15986a56"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class WNUTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
        "val_dataset = WNUTDataset(val_encodings, val_labels)"
      ],
      "id": "15986a56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01b073cf",
        "outputId": "481ba70d-0d77-46a3-d7be-d54a1e826a14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at HooshvareLab/distilbert-fa-zwnj-base were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
            "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/distilbert-fa-zwnj-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertForTokenClassification\n",
        "model = DistilBertForTokenClassification.from_pretrained(\"HooshvareLab/distilbert-fa-zwnj-base\", num_labels=len(unique_tags))"
      ],
      "id": "01b073cf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6bc8a67"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n"
      ],
      "id": "e6bc8a67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1023a049"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n"
      ],
      "id": "1023a049"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy0Y7HfXHcca"
      },
      "outputs": [],
      "source": [
        "# !pip install seqeval\n",
        "# !pip install datasets\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"seqeval\")"
      ],
      "id": "zy0Y7HfXHcca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFytxcq50NVQ"
      },
      "outputs": [],
      "source": [
        "u_tags =list(unique_tags)\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [u_tags[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [u_tags[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "id": "uFytxcq50NVQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8edf27f"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,             # evaluation dataset\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "id": "e8edf27f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wVlHxYNcvV2w",
        "outputId": "3ff4e495-95c1-4c93-92e5-a9314957bee1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 44285\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 8304\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8304/8304 1:47:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.048700</td>\n",
              "      <td>0.043130</td>\n",
              "      <td>0.921995</td>\n",
              "      <td>0.939136</td>\n",
              "      <td>0.930487</td>\n",
              "      <td>0.985994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.038400</td>\n",
              "      <td>0.040256</td>\n",
              "      <td>0.936934</td>\n",
              "      <td>0.942790</td>\n",
              "      <td>0.939853</td>\n",
              "      <td>0.987362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.024200</td>\n",
              "      <td>0.041852</td>\n",
              "      <td>0.935902</td>\n",
              "      <td>0.945430</td>\n",
              "      <td>0.940642</td>\n",
              "      <td>0.987561</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to ./results/checkpoint-500\n",
            "Configuration saved in ./results/checkpoint-500/config.json\n",
            "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-1500\n",
            "Configuration saved in ./results/checkpoint-1500/config.json\n",
            "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-2000\n",
            "Configuration saved in ./results/checkpoint-2000/config.json\n",
            "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-2500\n",
            "Configuration saved in ./results/checkpoint-2500/config.json\n",
            "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8858\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ye seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: y seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ve seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: @e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "Saving model checkpoint to ./results/checkpoint-3000\n",
            "Configuration saved in ./results/checkpoint-3000/config.json\n",
            "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-3500\n",
            "Configuration saved in ./results/checkpoint-3500/config.json\n",
            "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-4000\n",
            "Configuration saved in ./results/checkpoint-4000/config.json\n",
            "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-4500\n",
            "Configuration saved in ./results/checkpoint-4500/config.json\n",
            "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-5000\n",
            "Configuration saved in ./results/checkpoint-5000/config.json\n",
            "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-5500\n",
            "Configuration saved in ./results/checkpoint-5500/config.json\n",
            "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8858\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ye seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: y seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ve seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: @e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "Saving model checkpoint to ./results/checkpoint-6000\n",
            "Configuration saved in ./results/checkpoint-6000/config.json\n",
            "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-6500\n",
            "Configuration saved in ./results/checkpoint-6500/config.json\n",
            "Model weights saved in ./results/checkpoint-6500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-7000\n",
            "Configuration saved in ./results/checkpoint-7000/config.json\n",
            "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-7500\n",
            "Configuration saved in ./results/checkpoint-7500/config.json\n",
            "Model weights saved in ./results/checkpoint-7500/pytorch_model.bin\n",
            "Saving model checkpoint to ./results/checkpoint-8000\n",
            "Configuration saved in ./results/checkpoint-8000/config.json\n",
            "Model weights saved in ./results/checkpoint-8000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 8858\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ye seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: y seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ve seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: @e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8304, training_loss=0.05229067529133577, metrics={'train_runtime': 6469.8784, 'train_samples_per_second': 20.534, 'train_steps_per_second': 1.283, 'total_flos': 1.735917301435392e+16, 'train_loss': 0.05229067529133577, 'epoch': 3.0})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ],
      "id": "wVlHxYNcvV2w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "1d0d4c88",
        "outputId": "1f5b4833-9b85-410a-d70c-2b5f560f004c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 8858\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [139/139 02:32]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ye seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: ve seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: y seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: @e seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'epoch': 3.0,\n",
              " 'eval_accuracy': 0.9871010103354375,\n",
              " 'eval_f1': 0.9377926803983672,\n",
              " 'eval_loss': 0.04349938780069351,\n",
              " 'eval_precision': 0.9341261928500214,\n",
              " 'eval_recall': 0.9414880636223999,\n",
              " 'eval_runtime': 181.5843,\n",
              " 'eval_samples_per_second': 48.782,\n",
              " 'eval_steps_per_second': 0.765}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = trainer.evaluate()"
      ],
      "id": "1d0d4c88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvcTKWllFdo5"
      },
      "outputs": [],
      "source": [
        "res"
      ],
      "id": "UvcTKWllFdo5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubVZC5YcpdUn"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "ubVZC5YcpdUn"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BERT_Kasr_e_Ezafeh_Tagger.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}